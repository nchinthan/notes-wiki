<div class="text">
                                                                <p dir="ltr" style="text-align: start;"><span>Having too many features in data can cause problems like overfitting (good on training data but poor on new data), slower computation, and lower accuracy. This is called the&nbsp;</span><a href="https://www.geeksforgeeks.org/videos/curse-of-dimensionality-in-machine-learning/" rel="noreferrer noopener"><span>curse of dimensionality</span></a><span>, where more features exponentially increase the data needed for reliable results.</span></p>
<p dir="ltr"><span>The explosion of feature combinations makes sampling harder In high-dimensional data and tasks like </span><b><strong>clustering or classification more complex and slow.</strong></b><span>&nbsp;</span></p>
<blockquote><p dir="ltr" style="text-align: start;"><span>To tackle this problem, we use&nbsp;</span><b><strong>feature selection</strong></b><span>&nbsp;(choosing the most important features) and&nbsp;</span><b><strong>feature extraction</strong></b><span>&nbsp;(creating new features from the original ones). One popular feature extraction method is&nbsp;</span><a href="https://www.geeksforgeeks.org/dimensionality-reduction/" target="_blank" rel="noopener"><b><strong>dimensionality reduction</strong></b></a><span>, which reduces the number of features while keeping as much important information as possible.</span></p>
</blockquote><p dir="ltr" style="text-align: start;"><span>One of the most widely used dimensionality reduction techniques is&nbsp;</span><b><strong>Principal Component Analysis (PCA)</strong></b><span>.</span></p>
<h2 style="text-align:left"><span>How PCA Works for Dimensionality Reduction? </span></h2><p dir="ltr"><span>PCA is a statistical technique introduced by mathematician Karl Pearson in 1901. </span><i><b><strong class="GFGEditorTheme__textItalic">It works by transforming high-dimensional data into a lower-dimensional space while maximizing the variance (or spread) of the data in the new space</strong></b></i><span>. This helps preserve the most important patterns and relationships in the data.</span><i><b><strong class="GFGEditorTheme__textItalic"> </strong></b></i></p>
<blockquote><p dir="ltr"><i><b><strong class="GFGEditorTheme__textItalic">Note: It prioritizes the directions where the data varies the&nbsp;most&nbsp;(because more variation = more useful information.</strong></b></i></p>
</blockquote><p dir="ltr"><span>Let‚Äôs understand it‚Äôs working in simple terms:</span></p><div id="GFG_AD_Desktop_InContent_ATF_728x280" style="text-align: center; margin: 10px 0px; display: flex; justify-content: center; flex-wrap: wrap; gap: 1.5em;"><div id="GFG_AD_Desktop_InContent_ATF_300x250_1" style="width: 320px;"></div><div id="GFG_AD_Desktop_InContent_ATF_300x250_2" style="width: 320px;"></div></div>
<p dir="ltr"><span>Imagine you‚Äôre looking at a messy cloud of data points (like stars in the sky) and want to simplify it. PCA helps you find the ‚Äúmost important angles‚Äù to view this cloud so you don‚Äôt miss thes. Here‚Äôs how it works, step by step:</span></p>
<h3 style="text-align:left"><b><strong>Step 1: Standardize the Data</strong></b></h3><p dir="ltr"><span>Make sure all features (e.g., height, weight, age) are on the&nbsp;</span><b><strong>same scale</strong></b><span>. Why? A feature like ‚Äúsalary‚Äù (ranging 0‚Äì100,000) could dominate ‚Äúage‚Äù (0‚Äì100) otherwise.</span></p>
<p dir="ltr"><a href="https://www.geeksforgeeks.org/normalization-vs-standardization/" rel="noopener"><span>Standardizing</span></a><span> our dataset to ensures that each variable has a mean of 0 and a standard deviation of 1.</span></p>
<p style="text-align: left;"><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi><mo>=</mo><mfrac><mrow><mi>X</mi><mo>‚àí</mo><mi>Œº</mi></mrow><mi>œÉ</mi></mfrac></mrow><annotation encoding="application/x-tex">Z = \frac{X-\mu}{\sigma}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span cclass="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0715em;">Z</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.2694em; vertical-align: -0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.9244em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">œÉ</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.4461em;"><span class="pstru" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0785em;">X</span><span class="mbin mtight">‚àí</span><span class="mord mathnormal mtight">Œº</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p dir="ltr"><span>Here,</span></p>
<ul><li value="1"><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œº</mi></mrow><annotation encoding="application/x-tex">\mu






</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">Œº</span></span></span></span></span><span>& the mean of independent features &nbsp;</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œº</mi><mo>=</mo><mrow><mo fence="true">{</mo><msub><mi>Œº</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>Œº</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>‚ãØ</mo>‚Äâ<mo separator="true">,</mo><msub><mi>Œº</mi><mi>m</mi></msub><mo fence="true">}</mo></mrow></mrow><annotation encoding="application/x-tex">\mu = \left \{ \mu_1, \mu_2, \cdots, \mu_m \right \}






</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.625em; vertical-align: -0.1944em;"></span><span class="mord mathnormal">Œº</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span cner"><span class="mopen delimcenter" style="top: 0em;">{</span><span class="mord"><span class="mord mathnormal">Œº</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">Œº</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner">‚ãØ</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal">Œº</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class=t-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter" style="top: 0em;">}</span></span></span></span></span></span></li><li value="2"><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÉ</mi></mrow><annotation encoding="application/x-tex">\sigma




</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">œÉ</span></span></span></span></span><span> is the </span><a href="https://www.geeksforgeeks.org/mathematics-mean-variance-and-standard-deviation/" rel="noopener"><span>standard deviation</span></a><span> of independent features &nbsp;</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow>œÉ</mi><mo>=</mo><mrow><mo fence="true">{</mo><msub><mi>œÉ</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>œÉ</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>‚ãØ</mo>‚Äâ<mo separator="true">,</mo><msub><mi>œÉ</mi><mi>m</mi></msub><mo fence="true">}</mo></mrow></mrow><annotation encoding="application/x-tex">\sigma = \left \{ \sigma_1, \sigma_2, \cdots, \sigma_m \right \}




</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.4306em;"></span><span class="mord mathnormal" style="margin-right: 0.0359em;">œÉ</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;">{</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">œÉ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2pan></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="minner">‚ãØ</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">œÉ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class=r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter" style="top: 0em;">}</span></span></span></span></span></span></li></ul><h3 style="text-align:left"><b><strong>Step 2: Find Relationships</strong></b></h3><p dir="ltr"><span>Calculate how features&nbsp;</span><b><strong>move together</strong></b><span>&nbsp;using a&nbsp;</span><i><em class="GFGEditorTheme__textItalic">covariance matrix</em></i><span>. </span><a href="https://www.geeksforgeeks.org/mathematics-covariance-and-correlation/" rel="noopener"><span>Covariance</span></a><span> measures the strength of joint variability between two or more variables, indicating how much they change in relation to each other. To find the covariance we can use the formula:</span></p>
<p dir="ltr" style="text-align: center;"><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>o</mi><mi>v</mi><mo stretchy="false">(</mo><mi>x</mi><mn>1</mn><mo separator="true">,</mo><mi>x</mi><mn>2</mn><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msubsup><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo stretchy="false">(</mo><mi>x</mi><msub><mn>1</mn><mi>i</mi></msub><mo>‚àí</mo><mover accent="true"><mrow><mi>x</mi><mn>1</mn></mrow><mo>Àâ</mo></mover><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>x</mi><msub><mn>2</mn><mi>i</mi></msub><mo>‚àí</mo><mover accent="true"><mrow><mi>x</mi><mn>2</mn></mrow><mo>Àâ</mo></mover><mo stretchy="false">)</mo></mrow><mrow><mi>n</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">cov(x1,x2) = \frac{\sum_{i=1}^{n}(x1_i-\bar{x1})(x2_i-\bar{x2})}{n-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal">co</span><span class="mord mathnormal" style="margin-right: 0.0359en><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord mathnormal">x</span><span class="mord">2</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1.4852em; vertical-align: -0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.0819em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">‚àí</span><span class="mord mtight">1</span></span></span></span><span class="" styl"top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.535em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position: relative; top: 0em;">‚àë</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.7385em;"><span class="" style="top: -2.1786em; margin-left: 0em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="top: -2.931em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 sizemtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.3214em;"><span class=""></span></span></span></span></span></span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mord mtight"><span class="mord mtight">1</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3281em;"><span class="" style="top: -2.357em; margin-left: 0em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span><span class="mbin mtight">‚àí</span><span class="mord accent mtight"><span class="vlisspan class="vlist-r"><span class="vlist" style="height: 0.7812em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mord mtight">1</span></span></span><span class="" style="top: -2.9134em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.25em;"><span class="mord mtight">Àâ</span></span></span></span></span></span></span><span class="mclose mtight">)</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mord mtight"><span class="mord mtight">2</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3281em;"><span class="" style="top: -2.357em; margin-left: 0em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></pan><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span><span class="mbin mtight">‚àí</span><span class="mord accent mtight"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7812em;"><span class="" style="top: -2.7em;"><span class="pstrut" style="height: 2.7em;"></span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mord mtight">2</span></span></span><span class="" style="top: -2.9134em;"><span class="pstrut" style="height: 2.7em;"></span><span class="accent-body" style="left: -0.25em;"><span class="mord mtight">Àâ</span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.4033em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<p dir="ltr"><span>The value of covariance can be positive, negative, or zeros.</span></p>
<ul><li value="1"><b><strong>Positive: </strong></b><span>As the x1 increases x2 also increases.</span></li><li value="2"><b><strong>Negative:</strong></b><span> As the x1 increases x2 also decreases.</span></li><li value="3"><b><strong>Zeros: </strong></b><span>No direct relation. </span></li></ul><h3 style="text-align:left"><b><strong>Step 3: Find the ‚ÄúMagic Directions‚Äù (Principal Components)</strong></b></h3><ul><li value="1"><span>PCA identifies&nbsp;</span><b><strong>new axes</strong></b><span>&nbsp;(like rotating a camera) where the data spreads out the most:</span><ul><li value="1"><b><strong>1st Principal Component (PC1):</strong></b><span>&nbsp;The direction of maximum variance (most spread).</span></li><li value="2"><b><strong>2nd Principal Component (PC2):</strong></b><span>&nbsp;The next best direction,&nbsp;</span><i><em class="GFGEditorTheme__textItalic">perpendicular to PC1</em></ian>, and so on.</span></li></ul></li><li value="2"><span>These directions are calculated using&nbsp;</span><a href="https://www.geeksforgeeks.org/applications-of-eigenvalues-and-eigenvectors/#:~:text=Eigenvalues%20and%20eigenvectors%20are%20mathematical,scaled%20by%20its%20corresponding%20eigenvalue." target="_self" rel="noopener"><span>Eigenvalues and Eigenvectors</span></a><b><strong> where: eigenvectors</strong></b><span>&nbsp;(math tools that find these axes), and their importance is ranked by&nbsp;</span><b><strong>eigenvalues</strong></b><span>&nbsp;(how much variance each captures).</span></li></ul><p dir="ltr"><span>For a square matrix&nbsp;A, an&nbsp;</span><b><strong>eigenvector</strong></b><span>&nbsp;X&nbsp;(a non-zero vector) and its corresponding&nbsp;</span><b><strong>eigenvalue</strong></b><span>&nbsp;Œª&nbsp;(a scalar) satisfy:</span></p>
<p style="text-align: center;"><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A<mi><mi>X</mi><mo>=</mo><mi>Œª</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">AX = \lambda X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal">Œª</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span></span></span></span></span></p>
<p dir="ltr" style="text-align: start;"><span>This means:</span></p>
<ul><li value="1"><span>When&nbsp;</span><i><em class="GFGEditorTheme__textItalic">A</em></i><span>&nbsp;acts on&nbsp;X, it only stretches or shrinks&nbsp;X&nbsp;by the scalar&nbsp;Œª.</span></li><li value="2"><span>Thirection of&nbsp;X&nbsp;remains unchanged (hence, eigenvectors define ‚Äústable directions‚Äù of&nbsp;A).</span></li></ul><p dir="ltr"><span>It can also be written as :</span></p>
<p dir="ltr" style="text-align: center;"><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>A</mi><mi>X</mi><mo>‚àí</mo><mi>Œª</mi><mi>X</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mo stretchy="false">(</mo><mi>A</mi><mo>‚àí</mo><mi>Œª</mi><mi>I</mi><mo stretchy="false">)</mo><mi>X</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligambda X &amp;= 0 \\ (A-\lambda I)X &amp;= 0 \end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 3em; vertical-align: -1.25em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.75em;"><span class="" style="top: -3.91em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord mathnormal">Œª</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span></span></span><span class="" style="top: -2.41em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mopen">(</span><span classord mathnormal">A</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">‚àí</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord mathnormal">Œª</span><span class="mord mathnormal" style="margin-right: 0.0785em;">I</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 1.25em;"><span class=""></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.75em;"><span class="" style="top: -3.91em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord">0</span></span></span><span class="" style="top41em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord">0</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 1.25em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span></p>
<p dir="ltr"><span>where I is the identity matrix of the same shape as matrix A. And the above conditions will be true only if&nbsp;</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>A</mi><mtext>‚Äì</mtext><mi>Œª</mi><mi>I</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(A ‚Äì \lambda I)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mord">‚Äì</span><span class="mord mathnormal">Œª</span><span class="mord mathnormal" style="margin-right: 0.0785em;">I</span><span class="mclose">)</span></span></span></span></span><span>&nbsp;will be non-invertible (i.e. singular matrix). That means,</span></p>
<p style="text-align: center;"><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">‚à£</mi><mi>A</mi><mtext>‚Äì</mtext><mi>Œª</mi><mi>I</mi><mi mathvariant="normal">‚à£</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">|A ‚Äì \lambda I| = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord">‚à£</span><span class="mord mathnormal">A</span><span class="mordn class="mord mathnormal">Œª</span><span class="mord mathnormal" style="margin-right: 0.0785em;">I</span><span class="mord">‚à£</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">0</span></span></span></span></span></p>
<p dir="ltr"><span>This determinant equation is called the&nbsp;</span><b><strong>characteristic equation</strong></b><span>.</span></p>
<ul><li value="1"><span>Solving it gives the eigenvalues \lambda, </span></li><li value="2"><span>and therefore corresponding eigenvector can be found using the equation&nbsp;</span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>X</mi><mo>=</mo><mi>Œª</mi><mi>X</mi></mrow><annotation encoding="application/x-tex">AX = \lambda X</annotation></semantics></math></span><span class="k-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal">Œª</span><span class="mord mathnormal" style="margin-right: 0.0785em;">X</span></span></span></span></span><span>.</span></li></ul><blockquote><p dir="ltr"><b><strong>How This Connects to PCA</strong></b><span>?</span></p>
<ul><li value="1"><span>In PCA, the covariance matrix&nbsp;C (from Step 2) acts as matrix&nbsp;A.</span></li><li value="2"><span>Eigenvectors of&nbsp;</span><i><em class="GFGEditorTheme__textItalic">C</em></i><span>&nbsp;are the&nbsp;</span><b><strong>principal components</strong></b><span>&nbsp;(PCs).</span></li><li value="3"><span>Eigevalues&nbsp;represent the&nbsp;</span><b><strong>variance</strong></b><span>&nbsp;captured by each PC.</span></li></ul></blockquote><h3 style="text-align:left"><b><strong>Step 4: Pick the Top Directions &amp; Transform Data</strong></b></h3><ul><li value="1"><span>Keep only the top 2‚Äì3 directions (or enough to capture ~95% of the variance).</span></li><li value="2"><span>Project the data onto these directions to get a simplified, lower-dimensional version.</span></li></ul><p dir="ltr" style="text-align: start;"><span>PCA is an&nbsp;</span><a href="https://www.geeksforgeeks.org/supervised-unsupervised-learning/" rel="noopener"><span>unsupervised learning</span></a><b><strong> algorithm</strong></b><span>, meaning it doesn‚Äôt require prior knowledge of target variables. It‚Äôs commonly used in exploratory data analysis and machine learning to </span><b><strong>simplify datasets without losing critical information.</strong></b></p>
<blockquote><p dir="ltr"><b><strong>We know everything sound complicated, let‚rstand again with help of visual image where,</strong></b><span> </span><b><strong>x-axis (Radius)</strong></b><span> and </span><b><strong>y-axis (Area)</strong></b><span> represent two original features in the dataset.</span></p>
</blockquote><div style="width: 810px" class="wp-caption alignnone"><img src="https://media.geeksforgeeks.org/wp-content/uploads/20230420165431/Principal-Componenent-Analysisi.webp" alt="Principal Component Analysis - Geeksforgeeks" width="1000" height="inherit"><p class="wp-caption-text">Transform this 2D dataset into a 1D representation while preserving as much variance as possible.</p></div><p dir="ltr"><b><strong>Principal Components (PCs):</strong></b></p>
<ul><li value="1"><b><strong>PC‚ÇÅ (First Principal Component):</strong></b><span> The direction along which the data has the maximum variance. It captures the most important information.</span></li><li value="2"><b><strong>PC‚ÇÇ (Second Principal Component):</strong></b><span> The direction orthogonal (perpendicular) to PC‚ captures the remaining variance but is less significant.</span></li></ul><p dir="ltr"><span>Now, The </span><b><strong>red dashed lines</strong></b><span> indicate the spread (variance) of data along different directions . The variance along </span><b><strong>PC‚ÇÅ is greater than PC‚ÇÇ</strong></b><span>, which means that PC‚ÇÅ carries more useful information about the dataset.</span></p>
<ul><li value="1"><span>The data points (blue dots) are projected onto PC‚ÇÅ, effectively reducing the dataset from two dimensions (Radius, Area) to one dimension (PC‚ÇÅ).</span></li><li value="2"><span>This transformation simplifies the dataset while retaining most of the original variability.</span></li></ul><blockquote><p dir="ltr"><span>The image visually explains why </span><b><strong>PCA selects the direction with the highest variance</strong></b><span> (PC‚ÇÅ). By removing PC‚ÇÇ, we reduce redundancy while keeping essential information. The transformation helps in </span><b><strong>data compression, visualization, adel performance</strong></b><span>.</span></p>
</blockquote><h2 style="text-align:left"><span>Principal Component Analysis Implementation in Python</span></h2><p dir="ltr"><span>Hence, PCA employs a linear transformation that is based on preserving the most variance in the data using the least number of dimensions. It involves the following steps:</span></p>
<gfg-tabs data-run-ide="false" data-mode="dark" role="tablist">
            <gfg-tab slot="tab" hidden="" role="tab" id="gfg-tab-generated-0" aria-selected="true" tabindex="0" aria-controls="gfg-panel-generated-0" selected="">Python</gfg-tab>
<gfg-panel slot="panel" data-code-lang="python3" data-main-code-start="None" data-main-code-end="None" role="tabpanel" id="gfg-panel-generated-0" aria-labelledby="gfg-tab-generated-0">
    <code class="language-python3"><div class="highlight monokai"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Here we are using inbuilt dataset of scikit learn</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>

<span class="c1"># instantiating</span>
<span class="n">cancer</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># creating dataframe</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">cancer</span><span class="o">.</span><span class="n">frame</span>

<span class="c1"># checking shape</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Original Dataframe shape :'</span><span class="p">,</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Input features</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">cancer</span><span class="p">[</span><span class="s1">'feature_names'</span><span class="p">]]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Inputs Dataframe shape   :'</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div></code>
</gfg-panel></gfg-tabs><p dir="ltr"><b><strong>Output</strong></b><span>:</span></p>
<pre><span>Original Dataframe shape : (569, 31)
Inputs Dataframe shape   : (569, 30)</span></pre><p dir="ltr"><span>Now we will apply the first most step </span><b><strong>which is to standardize the data and for that, we will have to first calculate the mean and standard deviation of each feature in the feature space.</strong></b></p>
<gfg-tabs data-run-ide="false" data-mode="dark" role="tablist">
            <gfg-tab slot="tab" hidden="" role="tab" id="gfg-tab-generated-1" aria-selected="true" tabindex="0" aria-controls="gfg-panel-generated-1" selected="">Python</gfg-tab>
<gfg-panel slot="panel" data-code-lang="python3" data-main-code-start="None" data-main-code-end="None" role="tabpanel" id="gfg-panel-generated-1" aria-labelledby="gfg-tab-generated-1">
    <code class="language-python3"><div class="highlight monokai"><pre><span></span><span class="c1"># Mean</span>
<span class="n">X_mean</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Standard deviation</span>
<span class="n">X_std</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>

<span class="c1"># Standardization</span>
<span class="n">Z</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">X_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">X_std</span>
</pre></div></code>
</gfg-panel></gfg-tabs><p dir="ltr"><span>The </span><a href="https://www.geeksforgeeks.org/covariance-matrix/" target="_blank" rel="noopener"><span>covariance matrix</span></a><span> helps us visualize how strong the dependency of two features is with each other in the feature space.</span></p>
<gfg-tabs data-run-ide="false" data-mode="dark" role="tablist">
            <gfg-tab slot="tab" hidden="" role="tab" id="gfg-tab-generated-2" aria-selected="true" tabindex="0" aria-controls="gfg-panel-generated-2" selected="">Python</gfg-tab>
<gfg-panel slot="panel" data-code-lang="python3" data-main-code-start="None" data-main-code-end="None" role="tabpanel" id="gfg-panel-generated-2" aria-labelledby="gfg-tab-generated-2">
    <code class="language-python3"><div class="highlight monokai"><pre><span></span><span class="c1"># covariance</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">cov</span><span class="p">()</span>

<span class="c1"># Plot the covariance matrix</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div></code>
</gfg-panel></gfg-tabs><p dir="ltr"><b><strong>Output</strong></b><span>:</span></p>
<p dir="ltr"><img src="https://media.geeksforgeeks.org/wp-content/uploads/20230726123116/download-(2)-(1).webp" alt="Covariance Matrix (PCA)-Geeksforgeeks" width="640" height="531" loading="lazy"></p>
<p dir="ltr"><span>Now we will compute the </span><a href="https://www.geeksforgeeks.org/eigen-values/" rel="noopener"><span>eigenvectors</span></a><span> and </span><a href="https://www.geeksforgeeks.org/eigen-values/" rel="noopener"><span>eigenvalues</span></a><span> for our feature space which serve a great purpose in identifying the principal components for our feature space.</span></p>
<gfg-tabs data-run-ide="false" data-mode="dark" role="tablist">
            <gfg-tab slot="tab" hidden="" role="tab" id="gfg-tab-generated-3" aria-selected="true" tabindex="0" aria-controls="gfg-panel-generated-3" selected="">Python</gfg-tab>
<gfg-panel slot="panel" data-code-lang="python3" data-main-code-start="None" data-main-code-end="None" role="tabpanel" id="gfg-panel-generated-3" aria-labelledby="gfg-tab-generated-3">
    <code class="language-python3"><div class="highlight monokai"><pre><span></span><span class="n">eigenvalues</span><span class="p">,</span> <span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Eigen values:</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="n">eigenvalues</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Eigen values Shape:'</span><span class="p">,</span> <span class="n">eigenvalues</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Eigen Vector Shape:'</span><span class="p">,</span> <span class="n">eigenvectors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div></code>
</gfg-panel></gfg-tabs><p dir="ltr"><b><strong>Output</strong></b><span>:</span></p>
<pre><span>Eigen values:
 [1.32816077e+01 5.69135461e+00 2.81794898e+00 1.98064047e+00
 1.64873055e+00 1.20735661e+00 6.75220114e-01 4.76617140e-01
 4.16894812e-01 3.50693457e-01 2.93915696e-01 2.61161370e-01
 2.41357496e-01 1.57009724e-01 9.41349650e-02 7.98628010e-02
 5.93990378e-02 5.26187835e-02 4.94775918e-02 1.33044823e-04
 7.48803097e-04 1.58933787e-03 6.90046388e-03 8.17763986e-03
 1.54812714e-02 1.80550070e-02 2.43408378e-02 2.74394025e-02
 3.11594025e-02 2.99728939e-02]
Eigen values Shape: (30,)
Eigen Vector Shape: (30, 30)
</span></pre><p dir="ltr"><span>Sort the eigenvalues in descending order and sort the corresponding eigenvectors accordingly.</span></p>
<gfg-tabs data-run-ide="false" data-mode="dark" role="tablist">
            <gfg-tab slot="tab" hidden="" role="tab" id="gfg-tab-generated-4" aria-selected="true" tabindex="0" aria-controls="gfg-panel-generated-4" selected="">Python</gfg-tab>
<gfg-panel slot="panel" data-code-lang="python3" data-main-code-start="None" data-main-code-end="None" role="tabpanel" id="gfg-panel-generated-4" aria-labelledby="gfg-tab-generated-4">
    <code class="language-python3"><div class="highlight monokai"><pre><span></span><span class="c1"># Index the eigenvalues in descending order </span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Sort the eigenvalues in descending order </span>
<span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">eigenvalues</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

<span class="c1"># sort the corresponding eigenvectors accordingly</span>
<span class="n">eigenvectors</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="p">[:,</span><span class="n">idx</span><span class="p">]</span>
</pre></div></code>
</gfg-panel></gfg-tabs><p dir="ltr"><span>Explained variance is the term that gives us an idea of the amount of the total variance which has been retained by selecting the principal components instead of the original feature space.</span></p>
<gfg-tabs data-run-ide="false" data-mode="dark" role="tablist">
            <gfg-tab slot="tab" hidden="" role="tab" id="gfg-tab-generated-5" aria-selected="true" tabindex="0" aria-controls="gfg-panel-generated-5" selected="">Python</gfg-tab>
<gfg-panel slot="panel" data-code-lang="python3" data-main-code-start="None" data-main-code-end="None" role="tabpanel" id="gfg-panel-generated-5" aria-labelledby="gfg-tab-generated-5">
    <code class="language-python3"><div class="highlight monokai"><pre><span></span><span class="n">explained_var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span>
<span class="n">explained_var</span>
</pre></div></code>
</gfg-panel></gfg-tabs><p dir="ltr"><b><strong>Output</strong></b><span>:</span></p>
<pre><span>array([0.44272026, 0.63243208, 0.72636371, 0.79238506, 0.84734274,
       0.88758796, 0.9100953 , 0.92598254, 0.93987903, 0.95156881,
       0.961366  , 0.97007138, 0.97811663, 0.98335029, 0.98648812,
       0.98915022, 0.99113018, 0.99288414, 0.9945334 , 0.99557204,
       0.99657114, 0.99748579, 0.99829715, 0.99889898, 0.99941502,
       0.99968761, 0.99991763, 0.99997061, 0.99999557, 1.        ])
</span></pre><p dir="ltr"><b><strong>Determine the Number of Principal Components&nbsp;</strong></b></p>
<p dir="ltr"><span>Here we can either consider the number of principal components of any value of our choice or by limiting the explained variance. Here I am considering explained variance more than equal to 50%. Let‚Äôs check how many principal components come into this.</span></p>
<gfg-tabs data-run-ide="false" data-mode="dark" role="tablist">
            <gfg-tab slot="tab" hidden="" role="tab" id="gfgab-generated-6" aria-selected="true" tabindex="0" aria-controls="gfg-panel-generated-6" selected="">Python</gfg-tab>
<gfg-panel slot="panel" data-code-lang="python3" data-main-code-start="None" data-main-code-end="None" role="tabpanel" id="gfg-panel-generated-6" aria-labelledby="gfg-tab-generated-6">
    <code class="language-python3"><div class="highlight monokai"><pre><span></span><span class="n">n_components</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">explained_var</span> <span class="o">&gt;=</span> <span class="mf">0.50</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">n_components</span>
</pre></div></code>
</gfg-panel></gfg-tabs><p dir="ltr"><b><strong>Output</strong></b><span>:</span></p>
<pre><span>2
</span></pre><p dir="ltr"><b><strong>Project the Data onto the Selected Principal Components</strong></b></p>
<ul><li value="1"><span>Instead of storing full </span><b><strong>(x, y)</strong></b><span> coordinates, PCA </span><b><strong>stores only the projection values</strong></b><span> along the principal component, simplifying data processing.</span></li><li value="2"><span>Projection matrix: is a matrix of </span><b><strong>eigenvectors corresponding to the largest eigenvalues of the covariance matrix of the data</strong></b><span>. it projects the high-dimensional dataset onto a lower-dimensional subspace.</span></li></ul><gfg-tabs data-run-ide="false" data-mode="dark" role="tablist">
            <gfg-tab slot="tab" hidden="" role="tab" id="gfg-tab-generated-7" aria-selected="true" tabindex="0" aria-controls="gfg-panel-generated-7" selected="">Python</gfg-tab>
<gfg-panel slot="panel" data-code-lang="python3" data-main-code-start="None" data-main-code-end="None" role="tabpanel" id="gfg-panel-generated-7" aria-labelledby="gfg-tab-generated-7">
    <code class="language-python3"><div class="highlight monokai"><pre><span></span><span class="c1"># PCA component or unit matrix</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">eigenvectors</span><span class="p">[:,:</span><span class="n">n_components</span><span class="p">]</span>
<span class="n">pca_component</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">u</span><span class="p">,</span>
                             <span class="n">index</span> <span class="o">=</span> <span class="n">cancer</span><span class="p">[</span><span class="s1">'feature_names'</span><span class="p">],</span>
                             <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'PC1'</span><span class="p">,</span><span class="s1">'PC2'</span><span class="p">]</span>
                            <span class="p">)</span>

<span class="c1"># plotting heatmap</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">pca_component</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'PCA Component'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div></code>
</gfg-panel></gfg-tabs><p dir="ltr"><b><strong>Output</strong></b><span>:</span></p>

<p><img src="https://media.geeksforgeeks.org/wp-content/uploads/20230726123942/download-(4).webp" alt="Project the feature on Principal COmponent-Geeksforgeeks" width="594" height="605" loading="lazy"></p>
<p dir="ltr"><span>Then, we project our dataset using the formula: &nbsp;</span></p>
<p style="text-align: left;"><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>P</mi><mi>r</mi><mi>o</mi><msub><mi>j</mi><msub><mi>P</mi><mi>i</mi></msub></msub><mo stretchy="false">(</mo><mi>u</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mfrac><mrow><msub><mi>P</mi><mi>i</mi></msub><mo>‚ãÖ</mo><mi>u</mi></mrow><mrow><mi mathvariant="normal">‚à£</mi><mi>u</mi><mi mathvariant="normal">‚à£</mi></mrow></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><msub><mi>P</mi><mi>i</mi></msub><mo>‚ãÖ</mo><mi>u</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{alioj_{P_i}(u) &amp;= \frac{P_i\cdot u}{|u|} \\ &amp;=P_i\cdot u \end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 4.0963em; vertical-align: -1.7982em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.2982em;"><span class="" style="top: -4.2982em;"><span class="pstrut" style="height: 3.3603em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">P</span><span class="mord mathnormal">ro</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0572em;">j</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span class="" style="top: -2.55em; margin-left: -0.0572em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3281em;"><span class="" style="top: -2.357em; margin-left: -0.1389em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em;"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2501em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">u</span><span class="mclose">)</span></span></span><span class="" style="top: -2.2222em;"><span class="ut" style="height: 3.3603em;"></span><span class="mord"></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 1.7982em;"><span class=""></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.2982em;"><span class="" style="top: -4.2982em;"><span class="pstrut" style="height: 3.3603em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.3603em;"><span class="" style="top: -2.314em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord">‚à£</span><span class="mord mathnormal">u</span><span class="mord">‚à£</spann></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.677em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">‚ãÖ</span><span class="mspace" style="margin-right:222em;"></span><span class="mord mathnormal">u</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.936em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span class="" style="top: -2.2222em;"><span class="pstrut" style="height: 3.3603em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em;"><span class="" style="top: -2.55em; margin-left: -0.1389em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></sp></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">‚ãÖ</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mord mathnormal">u</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 1.7982em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span></p>
<div style="width: 810px" class="wp-caption alignnone"><img src="https://media.geeksforgeeks.org/wp-content/uploads/20230420165637/Finding-Projection-in-PCA.webp" alt="Finding Projection in PCA - Geeksforgeeks" width="1000" height="inherit" loading="lazy"><p class="wp-caption-text">Finding Projection in PCA</p></div><p dir="ltr"><span>The </span><b><strong>principal component u</strong></b><span> (green vector) maximizes data variance ands as the new axis for projection. The </span><b><strong>data point P1(x1,y1)</strong></b><span> (red vector) is an original observation, and its </span><b><strong>projection onto u</strong></b><span> (blue line) represents its transformed coordinate in the reduced dimension. This projection simplifies the data while preserving its key characteristics.</span></p>
<gfg-tabs data-run-ide="false" data-mode="dark" role="tablist">
            <gfg-tab slot="tab" hidden="" role="tab" id="gfg-tab-generated-8" aria-selected="true" tabindex="0" aria-controls="gfg-panel-generated-8" selected="">Python</gfg-tab>
<gfg-panel slot="panel" data-code-lang="python3" data-main-code-start="None" data-main-code-end="None" role="tabpanel" id="gfg-panel-generated-8" aria-labelledby="gfg-tab-generated-8">
    <code class="language-python3"><div class="highlight monokai"><pre><span></span><span class="c1"># Matrix multiplication or dot Product</span>
<span class="n">Z_pca</span> <span class="o">=</span> <span class="n">Z</span> <span class="o">@</span> <span class="n">pca_component</span>
<span class="c1"># Rename the columns name</span>
<span class="n">Z_pca</span><span class="o">.</span><span class="n">rename</span><span class="p">({</span><span class="s1">'PC1'</span><span class="p">:</span> <span class="s1">'PCA1'</span><span class="p">,</span> <span class="s1">'PC2'</span><span class="p">:</span> <span class="s1">'PCA2'</span><span class="p">},</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># Print the  Pricipal Component values</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Z_pca</span><span class="p">)</span>
</pre></div></code>
</gfg-panel></gfg-tabs><p dir="ltr"><b><strong>Output</strong></b><span>:</span></p>
<pre><span>          PCA1       PCA2
0     9.184755   1.946870
1     2.385703  -3.764859
2     5.728855  -1.074229
3     7.116691  10.266556
4     3.931842  -1.946359
..         ...        ...
564   6.433655  -3.573673
565   3.790048  -3.580897
566   1.255075  -1.900624
567  10.365673   1.670540
568  -5.470430  -0.670047
[569 rows x 2 columns]
</span></pre><blockquote><p dir="ltr"><span>The eigenvectors of the covariance matrix of the data are referred to as the principal axes of the data, </span><b><strong>and the projection of the data instances onto these principal axes are called the principal components. </strong></b></p>
</blockquote><p dir="ltr"><span>Dimensionality reduction is then obtained by only retaining those axes (dimensions) that account for most of the variance, and discarding all others.</span></p>
<h2 style="text-align:left"><span>PCA using Using Sklearn </span></h2><p dir="ltr"><span>There are different libraries in which the whole process of the principal component analysis has been automated by implementing it in a package as a function and we just have to pass the number of principal components which we would like to have. Sklearn is one such library that can be used for the PCA as shown below.</span></p>
<gfg-tabs data-run-ide="false" data-mode="dark" role="tablist">
            <gfg-tab slot="tab" hidden="" role="tab" id="gfg-tab-generated-9" aria-selected="true" tabindex="0" aria-controls="gfg-panel-generated-9" selected="">Python</gfg-tab>
<gfg-panel slot="panel" data-code-lang="python3" data-main-code-start="None" data-main-code-end="None" role="tabpanel" id="gfg-panel-generated-9" aria-labelledby="gfg-tab-generated-9">
    <code class="language-python3"><div class="highlight monokai"><pre><span></span><span class="c1"># Importing PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="c1"># Let's say, components = 2</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
<span class="n">x_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>

<span class="c1"># Create the dataframe</span>
<span class="n">df_pca1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">x_pca</span><span class="p">,</span>
                       <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">'PC</span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span>
                       <span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">)])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_pca1</span><span class="p">)</span>
</pre></div></code>
</gfg-panel></gfg-tabs><p dir="ltr"><b><strong>Output:</strong></b></p>
<pre><span>           PC1        PC2
0     9.184755   1.946870
1     2.385703  -3.764859
2     5.728855  -1.074229
3     7.116691  10.266556
4     3.931842  -1.946359
..         ...        ...
564   6.433655  -3.573673
565   3.790048  -3.580897
566   1.255075  -1.900624
567  10.365673   1.670540
568  -5.470430  -0.670047
[569 rows x 2 columns]
</span></pre><p dir="ltr"><span>We can match from the above Z_pca result from it is exactly the same values.</span></p>
<gfg-tabs data-run-ide="false" data-mode="dark" role="tablist">
            <gfg-tab slot="tab" hidden="" role="tab" id="gfg-tab-generated-10" aria-selected="true" tabindex="0" aria-controls="gfg-panel-generated-10" selected="">Python</gfg-tab>
<gfg-panel slot="panel" data-code-lang="python3" data-main-code-start="None" data-main-code-end="None" role="tabpanel" id="gfg-panel-generated-10" aria-labelledby="gfg-tab-generated-10">
    <code class="language-python3"><div class="highlight monokai"><pre><span></span><span class="c1"># giving a larger plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">c</span><span class="o">=</span><span class="n">cancer</span><span class="p">[</span><span class="s1">'target'</span><span class="p">],</span>
            <span class="n">cmap</span><span class="o">=</span><span class="s1">'plasma'</span><span class="p">)</span>

<span class="c1"># labeling x and y axes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'First Principal Component'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Second Principal Component'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div></code>
</gfg-panel></gfg-tabs><p dir="ltr"><b><strong>Output:</strong></b></p>
<p dir="ltr"><img src="https://media.geeksforgeeks.org/wp-content/uploads/20230726125541/download-(3)-(1).webp" alt="Visualizing the evaluated principal Component -Geeksforgeeks" width="702" height="525" loading="lazy"></p>
<gfg-tabs data-run-ide="false" data-mode="dark" role="tablist">
            <gfg-tab slot="tab" hidden="" role="tab" id="gfg-tab-generated-11" aria-selected="true" tabindex="0" aria-controls="gfg-panel-generated-11" selected="">Python</gfg-tab>
<gfg-panel slot="panel" data-code-lang="python3" data-main-code-start="None" data-main-code-end="None" role="tabpanel" id="gfg-panel-generated-11" aria-labelledby="gfg-tab-generated-11">
    <code class="language-python3"><div class="highlight monokai"><pre><span></span><span class="c1"># components</span>
<span class="n">pca</span><span class="o">.</span><span class="n">components_</span>
</pre></div></code>
</gfg-panel></gfg-tabs><p dir="ltr"><b><strong>Output</strong></b><span>:</span></p>
<pre><span>array([[ 0.21890244,  0.10372458,  0.22753729,  0.22099499,  0.14258969,
         0.23928535,  0.25840048,  0.26085376,  0.13816696,  0.06436335,
         0.20597878,  0.01742803,  0.21132592,  0.20286964,  0.01453145,
         0.17039345,  0.15358979,  0.1834174 ,  0.04249842,  0.10256832,
         0.22799663,  0.10446933,  0.23663968,  0.22487053,  0.12795256,
         0.21009588,  0.22876753,  0.25088597,  0.12290456,  0.13178394],
       [-0.23385713, -0.05970609, -0.21518136, -0.23107671,  0.18611302,
         0.15189161,  0.06016536, -0.0347675 ,  0.19034877,  0.36657547,
        -0.10555215,  0.08997968, -0.08945723, -0.15229263,  0.20443045,
         0.2327159 ,  0.19720728,  0.13032156,  0.183848  ,  0.28009203,
        -0.21986638, -0.0454673 , -0.19987843, -0.21935186,  0.17230435,
         0.14359317,  0.09796411, -0.00825724,  0.14188335,  0.27533947]])
</span></pre><p dir="ltr"><b><strong>Apart from what we‚Äôve discussed, there are many more subtle advantages and limitations to PCA.</strong></b></p>
<h2 style="text-align:left"><span>Advantages and Disadvantages of Principal Component Analysis</span></h2><p dir="ltr"><b><strong>Advantages of Principal Compent Analysis</strong></b></p>
<ol><li value="1"><b><strong>Multicollinearity Handling:</strong></b><span> Creates new, uncorrelated variables to address issues when original features are highly correlated.</span></li><li value="2"><b><strong>Noise Reduction:</strong></b><span> Eliminates components with low variance (assumed to be noise), enhancing data clarity.</span></li><li value="3"><b><strong>Data Compression:</strong></b><span> Represents data with fewer components, reducing storage needs and speeding up processing.</span></li><li value="4"><b><strong>Outlier Detection:</strong></b><span> Identifies unusual data points by showing which ones deviate significantly in the reduced space.</span></li></ol><p dir="ltr"><b><strong>Disadvantages of Principal Component Analysis</strong></b></p>
<ol><li value="1"><b><strong>Interpretation Challenges:</strong></b><span> The new components are combinations of original variables, which can be hard to explain.</span></li><li value="2"><b><strong>Data Scaling Sensitivity:</strong></b><span> Requires proper scaling of data before application, or results may be misleading.</span></li><li value="3"><b><strong>Information Loss:</strong></b><span> Reducing dimensions may lose some important information if too few components are kept.</span></li><li value="4"><b><strong>Assumption of Linearity:</strong></b><span> Works best when relationships between variables are linear, and may struggle with non-linear data.</span></li><li value="5"><b><strong>Computational Complexity:</strong></b><span> Can be slow and resource-intensive on very large datasets.</span></li><li value="6"><b><strong>Risk of Overfitting:</strong></b><span> Using too many components or working with a small dataset might lead to models that don‚Äôt generalize well.</span></li></ol><h2 style="text-align:left"><span>Conclusion</span></h2><p dir="ltr"><span>In summary, PCA helps in distilling complex data into its most informative elements, making it simpler and more efficient to analyze.</span></p>
<ol><li value="1">pan>It identifies the directions (called&nbsp;</span><b><strong>principal components</strong></b><span>) where the data varies the most.</span></li><li value="2"><span>It projects the data onto these directions, reducing the number of dimensions while retaining as much information as possible.</span></li><li value="3"><span>The new set of uncorrelated variables (principal components) is easier to work with and can be used for tasks like regression, classification, or visualization.</span></li></ol><div id="video-tab-content" class="video-tab-content">
                                        <div style="text-align: center; margin: 20px 0px;" id="GFG_AD_InContent_Desktop_728x280"></div>
                                                                                </div><div id="preFaqContent"></div><h2 style="text-align:left"><span>Principal Component Analysis(PCA) ‚Äì Frequently Asked Questions (FAQs)</span></h2><h3 style="text-align:left"><span>When should PCA be applied?</span></h3><blockquote><p dir="ltr"span>Using PCA is advantageous when working with multicollinear or high-dimensional datasets. Feature extraction, noise reduction, and data preprocessing are prominent uses for it.</span></p>
</blockquote><h3 style="text-align:left"><span>How are principal components interpreted?</span></h3><blockquote><p dir="ltr"><span>New axes are represented in the feature space by each principal component. An indicator of a component‚Äôs significance in capturing data variability is its capacity to explain a larger variance.</span></p>
</blockquote><h3 style="text-align:left"><span>What is the significance of principal components?</span></h3><blockquote><p dir="ltr"><span>&nbsp;data‚Äôs variance, allowing for a more concise representation.</span></p>
</blockquote><div hidead="MID"></div><div class="more-info hidden-div"><div class="article-meta-author-details "><div class="article-meta-author-details-block"><div class="article-meta-author-details-profile-display"><div class="author_info"><div class="article-meta-author-dls-profile-display-icon"><br></div></div></div></div></div><div class="bottom-wrap" style="margin-top: 12px; margin-bottom: 25px;"><div class="improved"><ul class="practice-tags">                                    </ul>
                                                                    </div>
                                                    </div>
                        </div>

                                                </div>